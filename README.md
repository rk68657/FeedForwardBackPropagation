# FeedForwardBackPropagation
Feed Forward Back Propagation Neural Network
#Simple Backpropagation Neural Network using NumPy

Play around with below Hyperparameter:
number of hidden layers & number of nodes in each layer
learning rate
the activation function (nonlinear vs linear, etc)
tweaking bias, input scaling, etc
the introduction of momentum to the backpropagation of deltas
'online' vs batch learing
using advanced optimization instead of backprop
modification of the cost function for adv. opt. including degree and type of regularization

This is just to clear the understanding how the networks learns and learns what, how the weights rearrange themself depending upon the feedback from output layer and how to create layers.

